# ìŒì‹ì  ë¦¬ë·° ê°ì„±ë¶„ì„ ëª¨ë¸ êµ¬ì¶• ê³¼ì • ë° ê²°ê³¼ ë¦¬í¬íŠ¸
ìŒì‹ì  ë¦¬ë·° ê°ì„±ë¶„ì„ ëª¨ë¸ êµ¬ì¶•, LSTM, beomi/KcELECTRA-base-v2022 ë¹„êµ

## ê°ì„±ë¶„ì„ ëª¨ë¸ í•™ìŠµ ê³„ê¸°


## ë°ì´í„° ì¤€ë¹„

### [ë¦¬ë·° ë°ì´í„° í¬ë¡¤ë§](https://github.com/tpdus751/naver_map_restaurant_review_crawl_process) 

### ë¦¬ë·° ë°ì´í„° ë¼ë²¨ë§
#### LM Studio API ê°ì •ë¶„ì„ ìš”ì²­ (Gemma 3 12B)
#### í”„ë¡¬í”„íŠ¸ êµ¬ì„±
```python
# âœ… ê°ì • ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
def make_batch_prompt(reviews):
    prompt = (
        "ë‹¹ì‹ ì€ ìŒì‹ì  ë¦¬ë·° ê°ì„± ë¶„ì„ê°€ ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¦¬ë·° ëª©ë¡ì„ í™•ì¸í•˜ê³ , ê° ë¦¬ë·°ì˜ ê°ì„±ì„ ìˆ«ìë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.\n"
        "- ë¶€ì •: 0\n- ì¤‘ë¦½: 1\n- ê¸ì •: 2\n"
        "í˜•ì‹: ìˆ«ìë§Œ ì‰¼í‘œë¡œ ë‚˜ì—´, ê¼­ ë¶€ì • 0, ì¤‘ë¦½ 1, ê¸ì • 2ë¥¼ ì§€í‚¬ ê²ƒ. ì˜ˆ: 2,1,0,2,1\n\n"
    )
    for i, review in enumerate(reviews, start=1):
        prompt += f"{i}. {str(review).strip()}\n"
    prompt += "\në‹µ:"
    return prompt

# âœ… ì‘ë‹µ íŒŒì‹±
def parse_response(text, expected_length):
    try:
        values = [int(x.strip()) if x.strip().isdigit() else None for x in text.strip().split(',')]
        if len(values) < expected_length:
            values += [None] * (expected_length - len(values))
        elif len(values) > expected_length:
            values = values[:expected_length]
        return values
    except Exception as e:
        print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return [None] * expected_length

# âœ… LM Studioë¡œ ê°ì • ë¶„ë¥˜ ìš”ì²­
def classify_batch_lmstudio(reviews):
    prompt = make_batch_prompt(reviews)
    try:
        response = requests.post(LM_STUDIO_API_URL, json={
            "prompt": prompt,
            "max_tokens": 200,
            "temperature": 0.0,
            "stop": ["\n"]
        })

        # âœ… ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸
        if response.status_code != 200:
            print(f"ğŸš« ì‘ë‹µ ì‹¤íŒ¨: HTTP {response.status_code}")
            print("ğŸ“© ì‘ë‹µ ë³¸ë¬¸:", response.text)
            return [None] * len(reviews)

        result = response.json()

        # âœ… ì‘ë‹µ êµ¬ì¡° í™•ì¸
        if "choices" not in result:
            print("ğŸš« ì‘ë‹µì— 'choices' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ“© ì „ì²´ ì‘ë‹µ:", result)
            return [None] * len(reviews)

        response_text = result['choices'][0]['text'].strip()
        print("ğŸ§¾ ì‘ë‹µ:", response_text[:100] + "...")

        return parse_response(response_text, len(reviews))

    except Exception as e:
        print(f"ğŸš« ìš”ì²­ ì‹¤íŒ¨: {e}")
        return [None] * len(reviews)
```

## ëª¨ë¸ í•™ìŠµ (LSTM, beomi/KcELECTRA-base-v2022)

### LSTM
#### ì½”ë“œ
ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
```

csv ë¶ˆëŸ¬ì˜¤ê¸°
```python
df = pd.read_csv('___.csv')  # 'text', 'label' ì»¬ëŸ¼ í¬í•¨
```

ê²°ì¸¡ì¹˜ ì œê±°
```python
df = df.dropna()
```

ì „ì²˜ë¦¬ ë° text, label ë¶„ë¦¬
```python
# ì „ì²˜ë¦¬
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = re.sub(r'\d+', '', text)      # ìˆ«ì ì œê±°
    return text.lower().strip()          # ì†Œë¬¸ì + ê³µë°± ì œê±°

# í…ìŠ¤íŠ¸, ë¼ë²¨ ë¶„ë¦¬
X = df['content'].astype(str).apply(clean_text)
y = df['label']
```

ë¼ë²¨ ë¶„í¬ í™•ì¸
```python
print("ë¼ë²¨ ë¶„í¬:\n", y.value_counts())
```
![image](https://github.com/user-attachments/assets/1b8d36a7-482b-403f-a68a-646811afaff8)

ë¼ë²¨ ìˆ˜ê°€ ì ì€ ë¶€ì •ì„ ê¸°ì¤€ìœ¼ë¡œ ê°¯ìˆ˜ í†µì¼
```python
# ë¼ë²¨ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ë‚˜ëˆ„ê¸°
df_0 = df[df['label'] == 0]
df_1 = df[df['label'] == 1].sample(n=20871, random_state=42)
df_2 = df[df['label'] == 2].sample(n=20871, random_state=42)

# ì„¸ í´ë˜ìŠ¤ í•©ì¹˜ê¸°
df_balanced = pd.concat([df_0, df_1, df_2]).sample(frac=1, random_state=42).reset_index(drop=True)

# ë¼ë²¨ ë¶„í¬ í™•ì¸
print("ê· í˜• ë§ì¶˜ ë¼ë²¨ ë¶„í¬:\n", df_balanced['label'].value_counts())
```
![image](https://github.com/user-attachments/assets/cbc565fc-69bb-4076-b14d-d7fb87eb08d3)

í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì…ë ¥ ë°ì´í„° ì •ìˆ˜ ì¸ì½”ë”©
```python
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)
```

íŒ¨ë”© (ëª¨ë¸ í•™ìŠµ ì‹œ ì…ë ¥ê°¯ìˆ˜ í†µì¼)
```python
max_len = 80
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')
```

ë¼ë²¨ ì›í•« ì¸ì½”ë”©
```python
num_classes = len(y.unique())  # ì˜ˆ: 3 (ë¶€ì •/ì¤‘ë¦½/ê¸ì •)
y_train_cat = to_categorical(y_train, num_classes=num_classes)
y_test_cat = to_categorical(y_test, num_classes=num_classes)
```

ëª¨ë¸ ì •ì˜
```python
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=max_len),
    LSTM(64, return_sequences=False),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.4),
    Dense(num_classes, activation='softmax')
])
```

ëª¨ë¸ ì»´íŒŒì¼ (optimizer, loss, metrics)
```python
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

ì–¼ë¦¬ìŠ¤íƒ€í•‘(val_loss ê¸°ì¤€ Epoch5 ë§Œí¼ ê°œì„ ì•ˆë˜ë©´ ë©ˆì¶¤)
```python
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)
```

ëª¨ë¸ í•™ìŠµ
```python
history = model.fit(
    X_train_pad, y_train_cat,
    epochs=50,
    batch_size=64,
    validation_split=0.2,
    callbacks=[early_stop]
)
```
![image](https://github.com/user-attachments/assets/0a292528-8aa9-4025-a8ef-bda0d92764b6)

### KcELECTRA-base-v2022
#### ì½”ë“œ
ë¼ì´ë¸ŒëŸ¬ë¦¬
```python
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, EarlyStoppingCallback, Trainer, DataCollatorWithPadding
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, ElectraConfig, ElectraForSequenceClassification
from sklearn.metrics import classification_report, f1_score
from sklearn.model_selection import train_test_split
from datasets import Value
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
```

csv ë¶ˆëŸ¬ì˜¤ê¸°
```python
review_df = pd.read_csv('./___.csv')
```

ë°ì´í„° ì „ì²˜ë¦¬
```python
cleaned_review = []

for review in review_df['content']:
  if len(review) < 2:
      review = np.nan
  else:
    if review == '':
      review = np.nan

    review = review.strip()

    if '\n' in review:
      review = review.replace('\n', '')

  cleaned_review.append(review)

review_df['content'] = cleaned_review

# ê²°ì¸¡ì¹˜ í™•ì¸
print(review_df.isnull().sum())

# ê²°ì¸¡ì¹˜ ì œê±°
review_df = review_df.dropna()

# ì¤‘ë³µì¹˜ í™•ì¸
print("ì¤‘ë³µ í–‰ ê°œìˆ˜:", review_df.duplicated().sum())

# ì¤‘ë³µì¹˜ ì œê±°
review_df = review_df.drop_duplicates()
```

ë¼ë²¨ ìˆ˜ê°€ ì ì€ ë¶€ì •ì„ ê¸°ì¤€ìœ¼ë¡œ ê°¯ìˆ˜ í†µì¼
```python
sample_size = review_df['label'].value_counts().min()
balanced_df = review_df.groupby('label').sample(n=sample_size, random_state=42)
balanced_df.groupby('label').size().reset_index(name='count')
```
![image](https://github.com/user-attachments/assets/efb68a82-c233-441b-bbd2-902f7bfc2c2c)

train, test ë°ì´í„°ì…‹ ë¶„ë¦¬
```python
df = balanced_df[['content', 'label']].copy()
df.columns = ['text', 'label']  # HuggingFace í˜•ì‹ì— ë§ê²Œ ì»¬ëŸ¼ëª… ë³€ê²½

# 2. train/test ë¶„ë¦¬ (stratify)
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))
```

í† í¬ë‚˜ì´ì € ë¡œë“œ
```python
model_name = "beomi/KcELECTRA-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

íŒ¨ë”© 
```python
def preprocess(example):
    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=35)

train_dataset = train_dataset.map(preprocess, batched=True)
test_dataset = test_dataset.map(preprocess, batched=True)
```

ë¼ë²¨ íƒ€ì… ì •ìˆ˜í˜•ìœ¼ë¡œ ì§€ì •
```python
train_dataset = train_dataset.cast_column("label", Value("int64"))
test_dataset = test_dataset.cast_column("label", Value("int64"))
```

config ì„¤ì • (ë“œëì•„ì›ƒ ê¸°ë³¸ê°’ 0.1 -> 0.3)
```python
config = ElectraConfig.from_pretrained(
    model_name,
    num_labels=3,
    hidden_dropout_prob=0.3,               # âœ… hidden layer dropout í™•ë¥  ì¡°ì •
    attention_probs_dropout_prob=0.3       # âœ… self-attention dropout í™•ë¥  ì¡°ì •
)
```

ëª¨ë¸ ìƒì„±
```python
model = ElectraForSequenceClassification.from_pretrained(
    model_name,
    config=config
)
```

í‰ê°€ì§€í‘œ ì •ì˜
```python
# 4. í‰ê°€ì§€í‘œ í•¨ìˆ˜ ì •ì˜
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    return {
        "accuracy": (preds == labels).mean(),
        "eval_f1_macro": f1_score(labels, preds, average="macro")
    }
```

í•™ìŠµ ì¸ì ì„¤ì •
```python
# 5. Trainer í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="steps",         
    eval_steps=250,                      # ğŸ”„ í‰ê°€ ì£¼ê¸° ì¡°ì • : 250
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1_macro",  # ğŸ”§ 'eval_' ë¶™ì—¬ì•¼ ë™ì‘í•¨
    greater_is_better=True,
    
    num_train_epochs=6,                  # âœ”ï¸ ì§§ê²Œ. EarlyStoppingë„ ìˆìœ¼ë‹ˆ overfitting ë°©ì§€
    per_device_train_batch_size=32,      
    per_device_eval_batch_size=64,

    learning_rate=1e-5,                  
    weight_decay=0.01,

    warmup_steps=1000,                    # âœ”ï¸ Warmup ì ìš© (ì˜ˆì—´ ë‹¨ê³„)
    logging_dir="./logs",
    logging_strategy="steps",
    logging_steps=100,                   # âœ”ï¸ ìì£¼ ë¡œê¹…í•˜ì—¬ ëª¨ë‹ˆí„°ë§

    report_to=[],                        # ğŸ”• tensorboard ë”
    seed=42
)
```
í•™ìŠµ ê°ì²´ ì„¤ì •
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]  # âœ”ï¸ patience ì¤„ì—¬ ë¹ ë¥¸ ì •ì§€ ìœ ë„
)
```

ëª¨ë¸ í•™ìŠµ
```python
trainer.train()
```
![image](https://github.com/user-attachments/assets/94f0ac83-5296-48fe-b9c9-78d371161a02)


## ê²°ê³¼

### Accuracy, Loss ê·¸ë˜í”„
#### LSTM
![image](https://github.com/user-attachments/assets/001f92cf-3e75-426b-b94e-43295a0e0e87)
Accuracy - LSTM
![image](https://github.com/user-attachments/assets/dcf198de-7612-4c10-bf9b-e56fa578204a)
Loss - LSTM

#### beomi/KcELECTRA-base-v2022
![image](https://github.com/user-attachments/assets/315164c0-ccc0-4a75-bd15-de365c7ffc13)
Accuracy - KcELECTRA
![image](https://github.com/user-attachments/assets/f27aebca-d113-4b79-8764-cf36cea34db0)
Loss - KcELECTRA

### classification_report

#### LSTM
![image](https://github.com/user-attachments/assets/e15b0bad-943b-4851-a6c7-b603c959c7d1)
classification_report - LSTM

#### beomi/KcELECTRA-base-v2022
![image](https://github.com/user-attachments/assets/20ced92a-38ef-435f-bafb-23f47f721665)
classification_report - KcELECTRA

Precision, Recall, F1-score ì „ì²´ì ìœ¼ë¡œ beomi/KcELECTRA-base-v2022 ìš°ìˆ˜

## beomi/KcELECTRA-base-v2022 í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
### í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„  ëª©í‘œ

config - dropout 0.3 -> 0.4 ì¡°ì ˆ
```python
config = ElectraConfig.from_pretrained(
    model_name,
    num_labels=3,
    hidden_dropout_prob=0.4,               # âœ… hidden layer dropout í™•ë¥  ì¡°ì •
    attention_probs_dropout_prob=0.4       # âœ… self-attention dropout í™•ë¥  ì¡°ì •
)
```

í´ë˜ìŠ¤ ë³„ ê°€ì¤‘ì¹˜ ì¡°ì ˆ (ì´ì „ KcELECTRA ëª¨ë¸ì€ ì¤‘ë¦½ì— ëŒ€í•œ ì„±ëŠ¥ì´ ì „ì²´ì ìœ¼ë¡œ ë‚®ìŒ -> ê°€ì¤‘ì¹˜ 2ë°° ë¶€ì—¬)
```python
from torch.nn import CrossEntropyLoss

# í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜: (ì˜ˆì‹œ) [ë¶€ì •:1.0, ì¤‘ë¦½:2.0, ê¸ì •:1.0]
class_weights = torch.tensor([1.0, 2.0, 1.0]).to(model_0624_5.device)

# Custom Trainer ì •ì˜
class WeightedLossTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")

        # GPUë¡œ class weight ë³´ë‚´ê¸°
        weight = class_weights.to(model.device)
        loss_fct = torch.nn.CrossEntropyLoss(weight=weight)
        loss = loss_fct(logits, labels)

        return (loss, outputs) if return_outputs else loss
```

í•™ìŠµ ì¸ì ìˆ˜ì •
```python
training_args = TrainingArguments(
    output_dir="./best_model",
    evaluation_strategy="steps",
    eval_steps=500,                 # 250 -> 500 steps
    save_strategy="steps",
    save_steps=500,                
    save_total_limit=2,             
    load_best_model_at_end=True,
    metric_for_best_model="macro_f1",
    greater_is_better=True,
    
    num_train_epochs=6,
    per_device_train_batch_size=64,  # ë°°ì¹˜ì‚¬ì´ì¦ˆ 32 -> 64 (ë¹ ë¥¸ í•™ìŠµ)
    per_device_eval_batch_size=64,
    
    learning_rate=3e-5,              # 1e-5 -> 3e-5 (ëª¨ë¸ì´ ë” ë¹ ë¥´ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸)
    weight_decay=0.01,
    warmup_steps=500,                # 1000 -> 500 (ì˜ˆì—´ êµ¬ê°„ ê°ì†Œ : í•™ìŠµ ì´ˆê¸°ì— í•™ìŠµë¥ (Learning Rate)ì„ ì²œì²œíˆ ì¦ê°€)
    logging_dir="./logs",
    logging_strategy="steps",
    logging_steps=100,              
    
    fp16=True,                      # GPU ìì›ì„ ëœ ì‚¬ìš©í•˜ê³  í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ëŠ” ê¸°ë²•
    report_to=[],                   
    seed=42
)
```

í•™ìŠµ ê°ì²´ ìˆ˜ì •
```python
from transformers import EarlyStoppingCallback
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,            
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),  
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # ê¸°ì¡´ 4 -> 3 (ë” ë¹ ë¥¸ ë©ˆì¶¤, ê³¼ì í•© ë°©ì§€)
)
```

í•™ìŠµ ì‹¤í–‰
```python
trainer.train()
```
![image](https://github.com/user-attachments/assets/d87a3bf5-4db6-439d-ada3-33292cadf1bc)

### beomi/KcELECTRA-base-v2022 Original vs New ì„±ëŠ¥ ë¹„êµ
#### Accuracy, Loss ê·¸ë˜í”„
![image](https://github.com/user-attachments/assets/315164c0-ccc0-4a75-bd15-de365c7ffc13)
Accuracy - KcELECTRA
![image](https://github.com/user-attachments/assets/f27aebca-d113-4b79-8764-cf36cea34db0)
Loss - KcELECTRA
